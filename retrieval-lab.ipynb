{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02c0edb-d8af-448e-94e9-17367d84e55d",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": [
    "# Retrieval Hands On Lab\n",
    "\n",
    "## Objectives\n",
    "By the end of this lab, participants will:\n",
    "\n",
    "1. Understand how to parse PDFs inside Snowflake\n",
    "2. Understand how to create vector representations of text data and load it into Snowflake tables\n",
    "3. Perform similarity search against embeddings in Snowflake\n",
    "4. Use Snowflake Cortex Search for retrieval and understand the benefits compared to simple similarity search\n",
    "5. Understand how to evaluate retrieval performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b997db5-7cf4-4c40-bab2-35a733171bdf",
   "metadata": {
    "collapsed": false,
    "name": "Repull"
   },
   "source": [
    "## Please re-pull the repo!\n",
    "Before we get too far in, re-pull the repo to be sure you have the latest updates. There may have been changes from when you initially created the repo connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6e170a",
   "metadata": {},
   "source": [
    "## Why focus on retrieval?\n",
    "LLMs are trained on mostly public information. If you go ask an LLM some specific question about a company's internal data, it won't know the answer and will likely make something up. In order to build effective enterprise applications, we need to provide the right data and context to the model to get meaningful results. While retrieval at its core is simple, there's a lot of nuance to be aware of when building an effective, scalable retrieval system.\n",
    "\n",
    "Thinking larger outside of GenAI and LLMs, retrieval also plays an important role in recommender systems and enterprise search solutions, meaning some the concepts covered here can be applied outside of just RAG applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065f51a-a7a7-4a0d-a686-208956c9639e",
   "metadata": {
    "collapsed": false,
    "name": "Part_1"
   },
   "source": [
    "# Part 1: Setup\n",
    "In this section, we will:\n",
    "\n",
    "1. Create some snowflake objects to store our data in\n",
    "2. Upload a PDF of Cincinnati Parks' 3 year development plan into a stage\n",
    "3. Parse the PDF into usable text and load the results into a Snowflake table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397eb24-3ccf-4c42-ae1b-a9f797a98758",
   "metadata": {
    "collapsed": false,
    "name": "Inspect_PDF"
   },
   "source": [
    "## Inspect the PDF\n",
    "\n",
    "Download the Cincinnati Parks 3 year plan by clicking on the data directory, then clicking the '...' next to the PDF and choosing 'Download'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550dc17e-125b-423b-9825-cc3cc5951cae",
   "metadata": {
    "collapsed": false,
    "name": "Create_Stage"
   },
   "source": [
    "## Create Stage and Upload PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "sql",
    "name": "Create_Snowflake_Objects"
   },
   "outputs": [],
   "source": [
    "USE RETRIEVAL_LAB.PUBLIC;\n",
    "\n",
    "-- Create a stage to store our PDF\n",
    "CREATE OR REPLACE STAGE docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d9541-9e4a-4446-a604-3dd5ef5d3969",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Put_PDF_In_Stage"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "session = get_active_session()\n",
    "pdf_path = \"./data/cincinnati-parks-3-year-plan.pdf\"\n",
    "\n",
    "session.file.put(\n",
    "    pdf_path,\n",
    "    \"@docs\",\n",
    "    auto_compress=False,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Force a refresh on the stage so the doc is accessible\n",
    "session.sql(\"ALTER STAGE docs REFRESH\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500453d9-a3f9-4e93-93dc-88f6b16df299",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Validate_File_In_Stage"
   },
   "outputs": [],
   "source": [
    "-- Verify PDF was properly uploaded\n",
    "LIST @docs;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76708aeb-13b2-4c34-afa4-6bba46985496",
   "metadata": {
    "collapsed": false,
    "name": "Parse_Document_Details"
   },
   "source": [
    "## Parse PDF\n",
    "We can use Snowflake Cortex's `PARSE_DOCUMENT` function to simply extract the text from our PDF file. `PARSE_DOCUMENT` takes a `mode` parameter that can be set as either `OCR` or `LAYOUT`. From the Snowflake docs:\n",
    "- `OCR` mode is the recommended option for quick, high-quality text extraction from documents such as manuals, agreement contracts, product detail pages, insurance policies and claims, and SharePoint documents.\n",
    "\n",
    "- `LAYOUT` mode is optimized for extracting text and layout elements like tables. This is the recommended option to improve the context of a document knowledge base to optimize retrieval information systems and for Large Language Model (LLM) inference.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We'll then insert the extracted text into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ac735-69a5-4b52-980b-d16b6369a833",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Create_Parsed_PDF_Table"
   },
   "outputs": [],
   "source": [
    "-- This table will store the text from the parsed PDF\n",
    "CREATE OR REPLACE TABLE PARSED_PDFS ( \n",
    "    RELATIVE_PATH VARCHAR,\n",
    "    SIZE NUMBER(38,0),\n",
    "    FILE_URL VARCHAR,\n",
    "    PARSED_DATA VARCHAR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4433273-b96e-460b-bf04-706a4d5e0661",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Parse_PDFs"
   },
   "outputs": [],
   "source": [
    "-- We use Snowflake Cortex's PARSE_DOCUMENT function to extract the text from the pdf and save it to a column\n",
    "INSERT INTO PARSED_PDFS (relative_path, size, file_url, parsed_data)\n",
    "SELECT \n",
    "        relative_path,\n",
    "        size,\n",
    "        file_url,\n",
    "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@docs', relative_path, { 'mode': 'OCR' }):content AS parsed_data\n",
    "    FROM directory(@docs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee574655-7368-41c8-8fcb-eaaeae3ceff0",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Verify_PDF_Parsing"
   },
   "outputs": [],
   "source": [
    "-- Verify the data was successfully parsed\n",
    "select * from PARSED_PDFS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be61e87-16c1-4406-9209-321a1d2363c9",
   "metadata": {
    "collapsed": false,
    "name": "Part2"
   },
   "source": [
    "## Part 2 - Generate Embeddings\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Explore various strategies for chunking the text data\n",
    "2. Generate embeddings for our text chunks\n",
    "3. Load the results into a Snowflake table using the `VECTOR` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36ab18-e357-41e5-995e-52b2a79f4d30",
   "metadata": {
    "collapsed": false,
    "name": "Chunking_Strategies"
   },
   "source": [
    "### Chunking Strategies\n",
    "\n",
    "In this section, we'll explore various chunking strategies. The right strategy will ultimately depend on the data and use case at hand. The three strategies explored in this section are:\n",
    "\n",
    "1. Snowflake's Recursive Text Splitter\n",
    "\n",
    "2. Semantic Chunking\n",
    "\n",
    "3. Paragraph Chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e034a-00ec-4c4f-be4c-e406056c8c73",
   "metadata": {
    "collapsed": false,
    "name": "Recursive_Text_Splitter_Details"
   },
   "source": [
    "### **Snowflake's Recursive Text Splitter**\n",
    "This method uses a set of separators to iteratively split the text. If the result of the split for the primary separator doesn't result in the desired chunk size, another pass will be taken through the text with the next separator. Snowflake's default separator list is `[”\\n\\n”, “\\n”, “ “, “”]`, meaning it will first use a paragraph break, then a line break, then a space, and then an empty string. This can be altered as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd35ad8-6208-4cfd-ac5a-d11573d12626",
   "metadata": {
    "language": "python",
    "name": "Snowflake_Recursive_Chunker"
   },
   "outputs": [],
   "source": [
    "chunking_sql = \"\"\"\n",
    "    SELECT f.value::string AS chunk\n",
    "    FROM PARSED_PDFS,\n",
    "    LATERAL FLATTEN(\n",
    "      INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
    "      PARSED_DATA, -- column to split\n",
    "      'none', -- format ('none' or 'markdown')\n",
    "      1000, -- chunk size\n",
    "      100 -- overlap\n",
    "      )\n",
    "    ) f\n",
    "\"\"\"\n",
    "\n",
    "recursive_chunks = [row[\"CHUNK\"] for row in session.sql(chunking_sql).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bb723-4d77-48b7-befe-8d9b91d0abca",
   "metadata": {
    "collapsed": false,
    "name": "Semantic_Chunking_Details"
   },
   "source": [
    "### Semantic Chunking\n",
    "\n",
    "This strategy attempts to generate chunks by accounting for the semantic meaning of text within the document. The goal is to create chunks comprised of sentences that cover the same theme or topic. This will result in more varied chunk sizes as opposed to always creating a chunk size of 500 tokens, as an example.\n",
    "\n",
    "This code utilizes a semantic chunkier built into LangChain. Since LangChain doesn't have native embedding support for Snowflake, we'll start by creating a custom Embeddings class for Snowflake embeddings that we can plug into the SemanticChunker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905dc98-c538-47a8-97da-3eb4f5ab8d32",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Semantic_Chunking"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from snowflake.cortex import embed_text_768\n",
    "\n",
    "\n",
    "# Creating a custom Langchain Embeddings class for Snowflake to use with the SemanticChunker\n",
    "class SnowflakeCortexEmbeddings(Embeddings):\n",
    "    def __init__(self):\n",
    "        self.model = 'e5-base-v2'\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            embedding = embed_text_768(self.model, text, session)\n",
    "            results.append(embedding)\n",
    "        return results\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "\n",
    "snowflake_embeddings = SnowflakeCortexEmbeddings()\n",
    "chunker = SemanticChunker(embeddings=snowflake_embeddings)\n",
    "\n",
    "parsed_data_df = session.table('parsed_pdfs')\n",
    "parsed_text = parsed_data_df.collect()[0]\n",
    "\n",
    "# Removing line breaks to pass in a continuous string to the chunker\n",
    "cleaned_text = re.sub(r'\\n(?=\\S)', ' ', parsed_text[\"PARSED_DATA\"])\n",
    "\n",
    "chunks = chunker.create_documents([cleaned_text])\n",
    "semantic_chunks = [doc.page_content for doc in chunks]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20c81f-8fe1-4b50-b335-5052f46e9a2e",
   "metadata": {
    "collapsed": false,
    "name": "Paragraph_Chunking_Details"
   },
   "source": [
    "### Paragraph Chunking\n",
    "This is a simpler strategy that can be used on documents that are well structured into paragraphs. In our case, this strategy makes a lot of sense as it will group each park plan into its own chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e3bf7-b2dc-4798-970f-cf5e9b067b9b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Paragraph_Chunking"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "def is_title(line):\n",
    "    stripped = line.strip()\n",
    "    return (\n",
    "        bool(stripped) and \n",
    "        stripped == stripped.upper() and \n",
    "        any(c.isalpha() for c in stripped)\n",
    "    )\n",
    "# Create chunks for each park and its project plan\n",
    "def chunk_by_project(parsed_text):\n",
    "    lines = parsed_text[\"PARSED_DATA\"].splitlines()\n",
    "    chunks = []\n",
    "    current_title = None\n",
    "    current_desc_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        if is_title(line):\n",
    "            # Check if the next line is also a title (part of same heading)\n",
    "            title_lines = [line]\n",
    "            while i + 1 < len(lines) and is_title(lines[i + 1].strip()):\n",
    "                i += 1\n",
    "                title_lines.append(lines[i].strip())\n",
    "            # If we already have a title and description, save that chunk\n",
    "            if current_title:\n",
    "                chunk = f\"{current_title}\\n{' '.join(current_desc_lines).strip()}\"\n",
    "                chunks.append({\n",
    "                    \"relative_path\": parsed_text[\"RELATIVE_PATH\"],\n",
    "                    \"size\": parsed_text[\"SIZE\"],\n",
    "                    \"file_url\": parsed_text[\"FILE_URL\"],\n",
    "                    \"chunk\": chunk\n",
    "                })\n",
    "            # Start a new chunk\n",
    "            current_title = ' '.join(title_lines)\n",
    "            current_desc_lines = []\n",
    "        else:\n",
    "            current_desc_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_title and current_desc_lines:\n",
    "        chunk = f\"{current_title}\\n{' '.join(current_desc_lines).strip()}\"\n",
    "        chunks.append({\n",
    "            \"relative_path\": parsed_text[\"RELATIVE_PATH\"],\n",
    "            \"size\": parsed_text[\"SIZE\"],\n",
    "            \"file_url\": parsed_text[\"FILE_URL\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "paragraph_chunks = chunk_by_project(parsed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bd002a-78b3-4aab-82e6-6f40883617b8",
   "metadata": {
    "collapsed": false,
    "name": "Chunk_Size_Details"
   },
   "source": [
    "### Compare chunking results\n",
    "To compare the different chunking strategies, we can look at the number of chunks created, alongside each chunks min, max, and avg tokens.\n",
    "\n",
    "Chunk size is an important factor. A smaller chunk size enables providing more diverse information to the model. Imagine the question at hand requires multiple different topics. If your chunk size is large, you may not be able to retrieve and fit a chunk for each topic. A smaller chunk size enables more chunks, meaning it can cover more topics.\n",
    "\n",
    "However, small chunk sizes also may result in loss of information. Since you're splitting the document into more pieces, an important passage may be broken into many chunks eliminating the relationship between the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e07375-caee-4dbe-9255-6daa5f43921d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Compare_Chunking_Results"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # This runs a SQL query to tokenize the text and count tokens\n",
    "    result = session.sql(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS('e5-base-v2', $${text}$$) AS token_count\n",
    "    \"\"\").collect()\n",
    "    return result[0][\"TOKEN_COUNT\"]\n",
    "\n",
    "def get_summary_statistics(chunks, label):\n",
    "    token_counts = [count_tokens(chunk if isinstance(chunk, str) else chunk[\"chunk\"]) for chunk in chunks]\n",
    "    return {\n",
    "        \"strategy\": label,\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts) if chunks else 0,\n",
    "    }\n",
    "\n",
    "all_summaries = [\n",
    "    get_summary_statistics(recursive_chunks, \"recursive\"),\n",
    "    get_summary_statistics(semantic_chunks, \"semantic\"),\n",
    "    get_summary_statistics(paragraph_chunks, \"paragraph\")\n",
    "]\n",
    "\n",
    "summary_df = pd.DataFrame(all_summaries)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc078b2-a5e2-4fe3-b378-6dbcaa0fa2fa",
   "metadata": {
    "language": "python",
    "name": "Investigate_Semantic_Large_chunk"
   },
   "outputs": [],
   "source": [
    "token_counts = [(chunk, count_tokens(chunk if isinstance(chunk, str) else chunk[\"chunk\"])) for chunk in semantic_chunks]\n",
    "for tc in token_counts:\n",
    "    if tc[1] == 1238:\n",
    "        print(tc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e76c82-0817-4686-b2bd-64099ca035fc",
   "metadata": {
    "collapsed": false,
    "name": "Chosen_Chunking_Strategy"
   },
   "source": [
    "### Chosen Strategy\n",
    "Given the organization of this document and the fact that none of the park sections are too large, simply breaking the document into chunks per paragraph is an ideal strategy. The rest of the code below could be ran for each chunking strategy to understand how it impacts performance.\n",
    "\n",
    "Let's investigate the paragraph chunks to ensure things were split as anticipated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80620465-147e-42a5-9a46-5be0db654b5f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "View_Chunk_Results"
   },
   "outputs": [],
   "source": [
    "# View paragraph chunks\n",
    "for idx, chunk in enumerate(paragraph_chunks):\n",
    "    if idx < 10:\n",
    "        print(f'chunk {idx}:', chunk['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8dbaa-baaf-4682-9c26-abb76494263a",
   "metadata": {
    "collapsed": false,
    "name": "Generate_Embeddings_Details"
   },
   "source": [
    "## Generate Embeddings\n",
    "Now that the data has been chunked, we can use embedding models inside of Cortex to create embeddings for our chunks of text. This converts the text data into vectors, enabling semantic similarity search to be performed against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6172931-178e-4170-878d-53eb03e43b2a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Generate_Embeddings"
   },
   "outputs": [],
   "source": [
    "from snowflake.cortex import embed_text_768\n",
    "\n",
    "# Create embeddings for each chunk\n",
    "model = 'e5-base-v2'\n",
    "for chunk in paragraph_chunks:\n",
    "    chunk['embedding'] = embed_text_768(model, chunk['chunk'], session)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209153f-9804-4dc1-9d79-4b37ff093a8d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Load_Embeddings"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import VectorType, DoubleType\n",
    "\n",
    "# Save off chunks and embeddings into new table\n",
    "df = session.create_dataframe(paragraph_chunks)\n",
    "df = df.with_column('embedding', df.col('embedding').cast(VectorType(float, 768)))\n",
    "df.write.save_as_table(\"DOCS_CHUNKS_TABLE\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e4a58-2caa-48a3-8968-dd8981b244c4",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Verify_Embeddings"
   },
   "outputs": [],
   "source": [
    "-- Validate table\n",
    "select chunk, embedding \n",
    "from DOCS_CHUNKS_TABLE\n",
    "limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b337bb-6f95-43db-b26f-733c98a3f6a3",
   "metadata": {
    "collapsed": false,
    "name": "Part_3"
   },
   "source": [
    "## Part 3: Test out different search methods\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Perform a standard cosine similarity search\n",
    "2. Create a Cortex Search Service\n",
    "3. Perform a search against the Cortex Search Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2451524-1339-46d0-bc9b-e2bfb37bb5b8",
   "metadata": {
    "collapsed": false,
    "name": "Standard_Search_Desc"
   },
   "source": [
    "### Standard Similarity Search\n",
    "Snowflake offers a built in function to perform semantic similarity search. You provide the column of type `VECTOR` that you will be searching and the embedding of your search query. It will return the similarity score (between -1 and 1) for each embedding in your vector column.\n",
    "\n",
    "One thing to note is that when inserting data into a column of type `VECTOR`, Snowflake does not create an Approximate Nearest Neighbor (ANN) index like you would get with many Vector databases. This will result in slower retrieval speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e1a83-ddd1-47b9-aae2-08d6648c8c3a",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Standard_Vector_Search"
   },
   "outputs": [],
   "source": [
    "-- Ault Park Trail\n",
    "SELECT VECTOR_COSINE_SIMILARITY(\n",
    "            docs_chunks_table.embedding,\n",
    "            SNOWFLAKE.CORTEX.EMBED_TEXT_768('e5-base-v2', 'When will the Ault Park trail plan complete?')\n",
    "       ) as similarity,\n",
    "       chunk\n",
    "FROM docs_chunks_table\n",
    "ORDER BY similarity desc\n",
    "LIMIT 10\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc84a433-8a02-40c8-9ea2-42430f043474",
   "metadata": {
    "collapsed": false,
    "name": "Cortex_Search_Desc"
   },
   "source": [
    "### Snowflake Cortex Search Service\n",
    "\n",
    "The Cortex Search Service provides a simple way to perform search against your data. It handles embedding data, loading it into a table, and provides a low-latency interface for searching. The search method is a hybrid of keyword and vector search and also has a built-in semantic reranker to provide the most relevant chunks of data.\n",
    "\n",
    "All of this together provides a simple mechanism for high quality, performant search inside Snowflake. Given the speed increase when compared to standard cosine similarity search, Snowflake is likely creating an ANN index for Cortex Search, although it is not explicitly called out in the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a54785-f0e9-48cc-8b87-bde2476a0960",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Create_Cortex_Search_Service"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE parks_search_service\n",
    "  ON CHUNK\n",
    "  WAREHOUSE = compute_wh\n",
    "  TARGET_LAG = '1 day'\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-m-v1.5'\n",
    "  AS (\n",
    "    SELECT\n",
    "        CHUNK,\n",
    "    FROM docs_chunks_table\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9276996-bca5-4498-9229-cf6739802f21",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Cortex_Search_Results"
   },
   "outputs": [],
   "source": [
    "# Quick test of the search service\n",
    "import json\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "root = Root(session)\n",
    "parks_search_service = (root\n",
    "  .databases[\"RETRIEVAL_LAB\"]\n",
    "  .schemas[\"PUBLIC\"]\n",
    "  .cortex_search_services[\"parks_search_service\"]\n",
    ")\n",
    "\n",
    "resp = parks_search_service.search(\n",
    "  query=\"When will the Ault Park trail plan complete?\",\n",
    "  columns=[\"chunk\"],\n",
    "  limit=3\n",
    ")\n",
    "\n",
    "results = json.loads(resp.to_json())['results']\n",
    "\n",
    "for idx, chunk in enumerate(results):\n",
    "    print(f'Result: {idx+1}')\n",
    "    print(chunk['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82843258-5703-4c77-a76e-2be53470c09b",
   "metadata": {
    "collapsed": false,
    "name": "Part_4"
   },
   "source": [
    "## Part 4: Evals\n",
    "\n",
    "In this section, we'll:\n",
    "\n",
    "1. Build out a sample of questions and ground truth results\n",
    "2. Create a framework for running our samples through both standard search and Cortex search\n",
    "3. Perform the evaluation and display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c03f03d-1263-47bd-9208-096641c884df",
   "metadata": {
    "collapsed": false,
    "name": "Eval_Overview"
   },
   "source": [
    "Typically, a framework such as [ragas](https://docs.ragas.io/en/stable/) would be used to perform the evaluation, but in order to keep the lab setup minimal, we are going to do the eval without the help of an external library.\n",
    "\n",
    "## Eval Metrics\n",
    "Our first eval set is going to be searching for a single specific chunk. Below is a set of search queries with the chunk we would expect to get back. Given this focused search effort, we'll use the following metrics for evaluation:\n",
    "\n",
    "- Hit@1 - determines if our search brought back the expected result as the first item retrieved\n",
    "- Hit@3 - measures if our expected chunk was present in the top 3 results\n",
    "- Mean Reciprocal Rank (MRR) - this metric is focused on how quickly the retrieval can find the first relevant result. Ex: if our expected chunk is at rank 1, then the reciprocal rank is 1. If our expected chunk is third, then the reciprocal rank is 1/3.\n",
    "- Miss rate - the rate at which the retrieval did not return our expected chunk at all.\n",
    "- P50/90/99 latency - this measures the speed of the retrieval results. P50 = indicates that 50% of the requests were returned in this time or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d196d1-b06d-4031-a876-3fd0cb692882",
   "metadata": {
    "language": "python",
    "name": "Create_Eval_Set"
   },
   "outputs": [],
   "source": [
    "exact_eval_set = [\n",
    "    {\n",
    "        \"query\": \"When will the Ault Park trail plan complete?\",\n",
    "        \"expected_chunk\": \"AULT PARK VALLEY TRAIL One of the busiest trails in Cincinnati Parks is experiencing serious erosion issues along the creek also housing important sewer infrastructure. This project shores up the trail to keep hikers safe in advance of a larger MSD sewer project in the coming years to protect the trail in the long term. The project is underway and will close out in early 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"When was the Sawyer Point Park playground burnt down?\",\n",
    "        \"expected_chunk\": \"SAWYER POINT PLAYGROUND AND PARK PLANNING Work is underway to restore the Sawyer Point Park playground, which was suddenly destroyed by a massive fire in November 2024. This project represents a chance to create an amazing new, uniquely Cincinnati, amenity for the next generation of park users of a wide range of ages and abilities to enjoy. The new playground will be built in the vicinity of the former playground though not in the same location. The goal is to engage with the community to develop something truly fantastic in this iconic regional park serving as a distinctive source of lasting pride for our city. The project also creates an opportunity to comprehensively review the layout of the park to guide a longer-term plan for improvements in the coming years.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How many miles is the CROWN network?\",\n",
    "        \"expected_chunk\": \"BRAMBLE PARK TRAIL This project is a partnership with the community and will utilize a State of Ohio Department of Natural Resources grant. This will be the first segment of the Little Duck Creek Trail and run 0.35 miles in length through Bramble Park in Madisonville. The trail will connect to the Murray Trail, part of the CROWN network connecting more than 104 miles of trails in Cincinnati. Planning will take place during 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"When did Smale park open?\",\n",
    "        \"expected_chunk\": \"SMALE CONCRETE & GRANITE UPGRADES This award-winning, heavily used signature Cincinnati Park opened in 2012. Sections of concrete and specialized granite need repair in order to maintain this regional asset.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which park was added to the National Register of Historic Places?\",\n",
    "        \"expected_chunk\": \"GIBSON HOUSE ROOF & FAÇADE This architecturally important structure was built in the middle 19th century and added to the National Register of Historic Places in 1976. Critical repairs are needed to preserve this treasure, which is now used for offices and a rental venue. Construction is planned to begin early 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who is the Park Board partnering with for the Smale River's Edge project?\",\n",
    "        \"expected_chunk\": \"SMALE RIVER’S EDGE The U.S. Army Corps of Engineers and the Cincinnati Park Board are partnering on a study to improve and revitalize the Cincinnati Ohio River’s edge along the western edge of Smale Riverfront Park. The overall vision is to make the Cincinnati Riverfront a welcoming, safe, sustainable park, serving as a gateway to connect people to their heritage, community, and the natural environment for generations to come. The project will provide opportunities for ecosystem restoration and recreation, while protecting Cincinnati’s Riverfront from erosion. Initial design selection of this multi-million project will be complete in mid-2025 with construction planned to start in 2027.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How long will the California Woods Hydrological plan take?\",\n",
    "        \"expected_chunk\": \"CALIFORNIA WOODS HYDROLOGICAL PLAN DESIGN This amazing preserve is experiencing significant erosion issues that threaten long-term public access to the park. A specialized firm has been selected to develop a plan for sustainable interventions to the stream flow to mitigate the on-going erosion issues in the most environmentally sustainable manner. Investigation and design work begins in the second quarter of 2025 and will take about a year to finalize.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which park plan is partering with the Cincinnati Off Road Alliance?\",\n",
    "        \"expected_chunk\": \"MT. AIRY BIKE SKILLS COURSE This partnership with the Cincinnati Parks Foundation and the Cincinnati Off Road Alliance (CORA) will nearly double the existing mileage of mountain biking trails within Mt. Airy Forest. It will be the first beginner natural surface trail experience within the city. With input from the community, the project has been funded and a contractor selected. The project is anticipated to be complete in early 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How many acres is the Cincinnati Park system comprised of?\",\n",
    "        \"expected_chunk\": \"CINCINNAT PARKS PARK IMPROVEMENT PROJECTS 3-YEAR PLAN Cincinnati Parks' 5,000 acres consist of 8 regional parks, 70 neighborhood parks, 34 preserves and natural areas, 5 parkways, 65 miles of hiking trails, 80,000 street trees on 1,000 miles of City streets, 6 nature centers, 18 scenic overlooks, 52 playgrounds, 500 landscaped gardens, and over 100 picnic areas. With all of this to care for, there are constant needs of all shapes and sizes. Whether it be a bad sidewalk, an aging playground, a leaking roof, or a park that could use a complete facelift, there’s plenty to do to keep our parks looking great and best serving our residents and users. This is why the Board of Park Commissioners approved a work plan, generated by Parks staff, outlining projects underway and planned over the next 3 years. This plan represents a roadmap of what Cincinnati Parks will be prioritizing in the coming years and creates transparency into improvement projects. This was developed after a careful evaluation based on a number of factors including safety, equity, efficiencies, long-term maintenance, available funding, and more. This plan represents current priorities, capacity, and needs, and is a living document that will be updated as circumstances evolve and schedules are adjusted.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which communities does the Burnet Woods dog park serve?\",\n",
    "        \"expected_chunk\": \"BURNET WOODS DOG PARK This new community dog park will serve Clifton, Corryville, CUF, and the surrounding areas, further contributing to the attractiveness and quality of life. The project represents a partnership with a number of community supporters, partners, and donors, including the Cincinnati Parks Foundation and Clifton Pop-up-Pup-Party (PUPP). The new amenity is expected to be under construction in May 2025 and take about 3 months to complete.\"\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dab75d-a4cc-4711-9287-d3a7c0f89464",
   "metadata": {
    "collapsed": false,
    "name": "Add_Eval_Question_Details"
   },
   "source": [
    "### Write your own eval question\n",
    "\n",
    "Building up a good set of ground truth questions is critical for evaluating AI applications. Take the time to read through the document and come up with a question or two, alongside the chunk you would expect to be returned. You can query the table `docs_chunks_table` to see the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4944c868-0af5-4a34-9c80-8547bfcedf6b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Add_Custom_Question"
   },
   "outputs": [],
   "source": [
    "# TODO: Add a question and expected chunk to the eval set\n",
    "query = \"\"\n",
    "expected_chunk = \"\"\n",
    "\n",
    "exact_eval_set.append({\"query\": query, \"expected_chunk\": expected_chunk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4707b-c54b-41f1-9541-4176eaa6e4b2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Eval_Setup"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "import json\n",
    "import re\n",
    "\n",
    "model = 'e5-base-v2'\n",
    "\n",
    "def escape_sql_string(s):\n",
    "    return s.replace(\"'\", \"''\")\n",
    "\n",
    "def vector_search(query, k=3):\n",
    "    safe_query = escape_sql_string(query)\n",
    "    return session.sql(f\"\"\"\n",
    "        SELECT chunk, VECTOR_COSINE_SIMILARITY(\n",
    "            docs_chunks_table.embedding,\n",
    "            SNOWFLAKE.CORTEX.EMBED_TEXT_768('{model}', '{safe_query}')\n",
    "        ) AS similarity\n",
    "        FROM docs_chunks_table\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT {k}\n",
    "    \"\"\").collect()\n",
    "\n",
    "def cortex_search(query, limit=3):\n",
    "    parks_search_service = (root.databases[\"RETRIEVAL_LAB\"]\n",
    "                                   .schemas[\"PUBLIC\"]\n",
    "                                   .cortex_search_services[\"parks_search_service\"])\n",
    "    resp = parks_search_service.search(\n",
    "        query=query,\n",
    "        columns=[\"chunk\"],\n",
    "        limit=limit\n",
    "    )\n",
    "    results = json.loads(resp.to_json())['results']\n",
    "    return results\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    # Lowercase, remove extra whitespace, and normalize newlines\n",
    "    return re.sub(r'\\s+', ' ', text.strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b9f8e-d836-487d-b204-75f62a69e8cc",
   "metadata": {
    "language": "python",
    "name": "Define_Eval_Functions"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_exact_eval():\n",
    "    results = []\n",
    "\n",
    "    for item in exact_eval_set:\n",
    "        query = item['query']\n",
    "        if query == \"\":\n",
    "            continue\n",
    "        expected = item['expected_chunk']\n",
    "\n",
    "        # Vector search with timing\n",
    "        start_time = time.time()\n",
    "        vector_results = vector_search(query, k=3)\n",
    "        vector_time = time.time() - start_time\n",
    "        vector_chunks = [r['CHUNK'] for r in vector_results]\n",
    "        vector_match_rank = next(\n",
    "            (i + 1 for i, chunk in enumerate(vector_chunks)\n",
    "             if normalize(chunk) == normalize(expected)),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        # Cortex search with timing\n",
    "        start_time = time.time()\n",
    "        cortex_results = cortex_search(query, limit=3)\n",
    "        cortex_time = time.time() - start_time\n",
    "        cortex_chunks = [r['chunk'] for r in cortex_results]\n",
    "        cortex_match_rank = next(\n",
    "            (i + 1 for i, chunk in enumerate(cortex_chunks)\n",
    "             if normalize(chunk) == normalize(expected)),\n",
    "            None\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"expected_chunk\": expected,\n",
    "            \"vector_hit_rank\": vector_match_rank or \"Miss\",\n",
    "            \"vector_chunks\": vector_chunks,\n",
    "            \"vector_time\": vector_time,\n",
    "            \"cortex_hit_rank\": cortex_match_rank or \"Miss\",\n",
    "            \"cortex_chunks\": cortex_chunks,\n",
    "            \"cortex_time\": cortex_time\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(results, method):\n",
    "    total = len(results)\n",
    "    hit_at_1 = 0\n",
    "    hit_at_3 = 0\n",
    "    reciprocal_ranks = []\n",
    "    ranks = []\n",
    "    misses = 0\n",
    "    times = []\n",
    "\n",
    "    for r in results:\n",
    "        hit_rank = r[f\"{method}_hit_rank\"]\n",
    "        response_time = r[f\"{method}_time\"]\n",
    "        times.append(response_time)\n",
    "\n",
    "        if isinstance(hit_rank, int):\n",
    "            if hit_rank == 1:\n",
    "                hit_at_1 += 1\n",
    "            if hit_rank <= 3:\n",
    "                hit_at_3 += 1\n",
    "            reciprocal_ranks.append(1.0 / hit_rank)\n",
    "            ranks.append(hit_rank)\n",
    "        else:\n",
    "            misses += 1\n",
    "            reciprocal_ranks.append(0.0)\n",
    "\n",
    "    accuracy = hit_at_1 / total\n",
    "    hit3 = hit_at_3 / total\n",
    "    mrr = sum(reciprocal_ranks) / total\n",
    "    mean_rank = sum(ranks) / len(ranks) if ranks else None\n",
    "    miss_rate = misses / total\n",
    "    avg_time = np.mean(times)\n",
    "\n",
    "    p50 = np.percentile(times, 50)\n",
    "    p90 = np.percentile(times, 90)\n",
    "    p99 = np.percentile(times, 99)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy (Hit@1)\": round(accuracy, 3),\n",
    "        \"Hit@3\": round(hit3, 3),\n",
    "        \"MRR\": round(mrr, 3),\n",
    "        \"Miss Rate\": round(miss_rate, 3),\n",
    "        \"Avg Retrieval Time (ms)\": round(avg_time * 1000, 3),\n",
    "        \"P50 Latency (ms)\": round(p50 * 1000, 3),\n",
    "        \"P90 Latency (ms)\": round(p90 * 1000, 3),\n",
    "        \"P99 Latency (ms)\": round(p99 * 1000, 3)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21487d7e-867c-48dd-8ab4-5109facbb828",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Run_Eval"
   },
   "outputs": [],
   "source": [
    "results = run_exact_eval()\n",
    "\n",
    "vector_metrics = compute_metrics(results, method=\"vector\")\n",
    "cortex_metrics = compute_metrics(results, method=\"cortex\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": list(vector_metrics.keys()),\n",
    "    \"Vector Search\": list(vector_metrics.values()),\n",
    "    \"Cortex Search\": list(cortex_metrics.values())\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb87f5-2404-402e-88fd-7d8755ca6e0e",
   "metadata": {
    "collapsed": false,
    "name": "Eval_2_Overview"
   },
   "source": [
    "### Another round of evals\n",
    "The first set of eval questions was looking for exact matches. While that is one type of question we can expect, how about more open ended questions that will rely on multiple chunks? The following sample queries were chosen to minimize exact keyword matching and to evaluate how many relevant chunks were returned in the top 3 results.\n",
    "\n",
    "For these questions, we will evaluate:\n",
    "- Precision@k - the ratio of relevant chunks retrieved to the total number of chunks retrieved\n",
    "- Recall@k - the ratio of relevant chunks retrieved to the total number of relevant chunks\n",
    "\n",
    "\n",
    "Determining the value of `k` is an important consideration when evaluating retrieval performance. In our question set below, the first question has 7 relevant chunks, while the other two have only 3 each.\n",
    "\n",
    "If we use `k=3`, then the maximum possible `recall@3` for the first question is limited to 3/7, even if the system retrieves the best results. On the other hand, if we use `k=7`, then the maximum possible `precision@7` for the second and third questions is only 3/7, since there are only 3 relevant chunks to be found.\n",
    "\n",
    "To balance the tradeoff between precision and recall across questions of varying difficulty, we’ll evaluate performance at multiple values of `k: [3, 5, 7]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07f316-c9d9-4bf5-8b52-d9702552f130",
   "metadata": {
    "language": "python",
    "name": "Create_Open_Eval_Set"
   },
   "outputs": [],
   "source": [
    "open_eval_set = [\n",
    "    {\n",
    "        \"query\": \"Which parks will have new slides within the next year?\",\n",
    "        \"expected_chunks\": [\n",
    "            \"AULT PARK PLAYGROUND Parks is working with the Ault Park Advisory Council (APAC), who are fundraising to supplement State of Ohio grant funding on this new playground. With the help of the community, the project has been designed. It is anticipated work will begin in the fall of 2025 with completion before the end of the year.\",\n",
    "            \"SAWYER POINT PLAYGROUND AND PARK PLANNING Work is underway to restore the Sawyer Point Park playground, which was suddenly destroyed by a massive fire in November 2024. This project represents a chance to create an amazing new, uniquely Cincinnati, amenity for the next generation of park users of a wide range of ages and abilities to enjoy. The new playground will be built in the vicinity of the former playground though not in the same location. The goal is to engage with the community to develop something truly fantastic in this iconic regional park serving as a distinctive source of lasting pride for our city. The project also creates an opportunity to comprehensively review the layout of the park to guide a longer-term plan for improvements in the coming years.\",\n",
    "            \"GLENWAY PARK RENOVATIONS & PLAYGROUND Parks is working with the Cincinnati Parks Foundation and the surrounding community to renovate Glenway Park in East Price Hill. Fundraising and conceptual planning are in development and include a new playground, new lighting, regrading, and walking path improvements to increase accessibility and visibility. Construction is slated to begin mid-2026.\",\n",
    "            \"MCEVOY PARK IMPROVEMENTS This park renovation project looks to implement traffic calming measures, a new playground, pavilion renovations and some trail improvements. Planning will kick off late 2025 and is anticipated to last a year.\",\n",
    "            \"MT. AIRY MCFARLAN PLAYGROUND Parks is working on developing a new signature adventure playground in Mt. Airy Forest off of McFarlan. The playground project would correspond with retirement of the Area 3 playground near the disc golf course that is at the end of its lifecycle. Planning for this project is planned to kick off in 2026.\",       \n",
    "            \"HOFFNER PARK PLAYGROUND This playground is due for replacement. Work will begin in mid-2026.\"\n",
    "            \"MLK PARK RENOVATION Parks will work with the community to design a renovated park best serving the surrounding neighborhoods. Planning with the community for this project is expected to be completed in 2027.\"\n",
    "        ]\n",
    "    },\n",
    "        {\n",
    "        \"query\": \"Which bicycling paths have updates planned?\",\n",
    "        \"expected_chunks\": [\n",
    "            \"TED BERRY INTERNATIONAL FRIENDSHIP PARK RESTORATION This park has been in a state of disrepair due to an emergency fix to a major water main running through the park. This project returns the original state of the park and is a joint venture between the Cincinnati Park Board and Greater Cincinnati Water Works. Key project elements include restoring bike paths, walkways, seating walls, pavers, new trees, sod, shrubs, electrical, lighting, irrigation, and more. This magnificent park is slated to be fully restored in May 2025.\",\n",
    "            \"MT. AIRY BIKE SKILLS COURSE This partnership with the Cincinnati Parks Foundation and the Cincinnati Off Road Alliance (CORA) will nearly double the existing mileage of mountain biking trails within Mt. Airy Forest. It will be the first beginner natural surface trail experience within the city. With input from the community, the project has been funded and a contractor selected. The project is anticipated to be complete in early 2026.\",\n",
    "            \"OHIO RIVER WAY BIKE TRAIL EXTENSION THROUGH SAWYER POINT & YEATMAN’S COVE This partnership with Great Parks, the City of Cincinnati, and Metro will design and construct a 4.75-mile shared use trail. It will go from Sawyer Point on the Cincinnati riverfront east to the Lunken Trail and the 78-mile Little Miami Scenic Trail. Planning is underway in 2025.\"\n",
    "        ]\n",
    "        \n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What updates are planned for Smale Park?\",\n",
    "        \"expected_chunks\": [\n",
    "            \"SMALE LOT 23 ENHANCEMENTS This project is a funding partnership with the Cincinnati Parks Foundation and has been in the works for several years. It includes adding large garden planters, granite promenade seat walls, a sandstone and granite donor wall and cladding walls. Construction will begin in late 2025 with completion in 2026.\",\n",
    "            \"SMALE RIVER’S EDGE The U.S. Army Corps of Engineers and the Cincinnati Park Board are partnering on a study to improve and revitalize the Cincinnati Ohio River’s edge along the western edge of Smale Riverfront Park. The overall vision is to make the Cincinnati Riverfront a welcoming, safe, sustainable park, serving as a gateway to connect people to their heritage, community, and the natural environment for generations to come. The project will provide opportunities for ecosystem restoration and recreation, while protecting Cincinnati’s Riverfront from erosion. Initial design selection of this multi-million project will be complete in mid-2025 with construction planned to start in 2027.\",\n",
    "            \"SMALE CONCRETE & GRANITE UPGRADES This award-winning, heavily used signature Cincinnati Park opened in 2012. Sections of concrete and specialized granite need repair in order to maintain this regional asset.\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c662f-d92b-403c-bfdf-62ecca7fd608",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Define_Open_Eval_metrics"
   },
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    hits = sum(normalize(r) in [normalize(chunk) for chunk in relevant] for r in retrieved_k)\n",
    "    return hits / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    hits = sum(normalize(r) in [normalize(chunk) for chunk in relevant] for r in retrieved_k)\n",
    "    return hits / len(relevant) if relevant else 0\n",
    "\n",
    "\n",
    "def run_open_eval(k):\n",
    "    results = []\n",
    "\n",
    "    for item in open_eval_set:\n",
    "        query = item['query']\n",
    "        relevant_chunks = item['expected_chunks']\n",
    "\n",
    "        # Vector search\n",
    "        start_time = time.time()\n",
    "        vector_results = vector_search(query, k=k)\n",
    "        vector_time = time.time() - start_time\n",
    "        vector_chunks = [r['CHUNK'] for r in vector_results]\n",
    "\n",
    "        # Cortex search\n",
    "        start_time = time.time()\n",
    "        cortex_results = cortex_search(query, limit=k)\n",
    "        cortex_time = time.time() - start_time\n",
    "        cortex_chunks = [r['chunk'] for r in cortex_results]\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"relevant_chunks\": relevant_chunks,\n",
    "            \"vector_chunks\": vector_chunks,\n",
    "            \"vector_time\": vector_time,\n",
    "            \"cortex_chunks\": cortex_chunks,\n",
    "            \"cortex_time\": cortex_time\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d6ce0-7970-494f-89ea-98fdd15cf586",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Compute_Open_Eval_Metrics"
   },
   "outputs": [],
   "source": [
    "def compute_open_metrics(results, method, k=3):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    times = []\n",
    "\n",
    "    for r in results:\n",
    "        relevant = r[\"relevant_chunks\"]\n",
    "        retrieved = r[f\"{method}_chunks\"]\n",
    "        response_time = r[f\"{method}_time\"]\n",
    "        times.append(response_time)\n",
    "\n",
    "        p = precision_at_k(retrieved, relevant, k)\n",
    "        rcl = recall_at_k(retrieved, relevant, k)\n",
    "\n",
    "        precisions.append(p)\n",
    "        recalls.append(rcl)\n",
    "\n",
    "    avg_p = np.mean(precisions)\n",
    "    avg_r = np.mean(recalls)\n",
    "    avg_time = np.mean(times)\n",
    "\n",
    "    p50 = np.percentile(times, 50)\n",
    "    p90 = np.percentile(times, 90)\n",
    "    p99 = np.percentile(times, 99)\n",
    "\n",
    "    return {\n",
    "        f\"Avg Precision@{k}\": round(avg_p, 3),\n",
    "        f\"Avg Recall@{k}\": round(avg_r, 3),\n",
    "        \"Avg Retrieval Time (ms)\": round(avg_time * 1000, 3),\n",
    "        \"P50 Latency (ms)\": round(p50 * 1000, 3),\n",
    "        \"P90 Latency (ms)\": round(p90 * 1000, 3),\n",
    "        \"P99 Latency (ms)\": round(p99 * 1000, 3)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f90ccc-3ae4-42a8-8588-f7c16c95e791",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Run_Open_Eval"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ks = [3, 5, 7]\n",
    "methods = [\"vector\", \"cortex\"]\n",
    "\n",
    "rows = []\n",
    "\n",
    "for k in ks:\n",
    "    open_results_k = run_open_eval(k=k)  # Run retrieval at this k\n",
    "    for method in methods:\n",
    "        metrics = compute_open_metrics(open_results_k, method=method, k=k)\n",
    "        for metric_name, value in metrics.items():\n",
    "            rows.append({\n",
    "                \"k\": k,\n",
    "                \"Method\": method.capitalize(),\n",
    "                \"Metric\": metric_name,\n",
    "                \"Value\": value\n",
    "            })\n",
    "\n",
    "# Create the DataFrame\n",
    "df_open = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: Pivot for better readability\n",
    "df_pivot = df_open.pivot(index=[\"Metric\", \"k\"], columns=\"Method\", values=\"Value\").reset_index()\n",
    "\n",
    "df_pivot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "dschuler@gaig.com",
   "authorId": "2986287304867",
   "authorName": "DSCHULER",
   "lastEditTime": 1749469201981,
   "notebookId": "ns22ir2pebtpjbv4m6ui",
   "sessionId": "941d471d-4456-432a-abd5-ea35b49262c6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
