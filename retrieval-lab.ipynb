{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "3swvgx32nrloin5a527w",
   "authorId": "9042333050115",
   "authorName": "THEAIPROGRAMMER3",
   "authorEmail": "theaiprogrammer3@gmail.com",
   "sessionId": "422eb8fe-fc83-4bff-a7c2-ddc4aa449c0b",
   "lastEditTime": 1747677100580
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02c0edb-d8af-448e-94e9-17367d84e55d",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": [
    "# Retrieval Hands On Lab\n",
    "\n",
    "## Objectives\n",
    "By the end of this lab, participants will:\n",
    "\n",
    "1. Understand how to parse PDFs inside Snowflake\n",
    "2. Understand how to create vector representations of text data and load it into Snowflake tables\n",
    "3. Perform similarity search against embeddings in Snowflake\n",
    "4. Use Snowflake Cortex Search for retrieval and understand the benefits compared to simple similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065f51a-a7a7-4a0d-a686-208956c9639e",
   "metadata": {
    "collapsed": false,
    "name": "Part_1"
   },
   "source": [
    "# Part 1: Setup\n",
    "In this section, we will:\n",
    "\n",
    "1. Create some snowflake objects to store our data in\n",
    "2. Upload a PDF of Cincinnati Parks' 3 year development plan into a stage\n",
    "3. Parse the PDF into usable text and load the results into a Snowflake table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397eb24-3ccf-4c42-ae1b-a9f797a98758",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Inspect the PDF\n\nDownload the Cincinnati Parks 3 year plan by clicking on the data directory, then clicking the '...' next to the PDF and choosing 'Download'."
  },
  {
   "cell_type": "markdown",
   "id": "75bd5177-ff2e-4e69-95b3-3f28e9010938",
   "metadata": {
    "collapsed": false,
    "name": "Add_Python_Packages"
   },
   "source": [
    "## Add python packages\n",
    "\n",
    "Click on 'Packages' at the top of the notebook and add the following packages:\n",
    "\n",
    "- snowflake.core\n",
    "- snowflake-snowpark-python\n",
    "- snowflake-ml-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "sql",
    "name": "Create_Snowflake_Objects"
   },
   "outputs": [],
   "source": "USE GAIG_LAB.PUBLIC;\n\n-- Create a stage to store our PDF\nCREATE OR REPLACE STAGE docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d9541-9e4a-4446-a604-3dd5ef5d3969",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Put_PDF_In_Stage"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "session = get_active_session()\n",
    "pdf_path = \"./data/cincinnati-parks-3-year-plan.pdf\"\n",
    "\n",
    "session.file.put(\n",
    "    pdf_path,\n",
    "    \"@docs\",\n",
    "    auto_compress=False,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Force a refresh on the stage so the doc is accessible\n",
    "session.sql(\"ALTER STAGE docs REFRESH\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500453d9-a3f9-4e93-93dc-88f6b16df299",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Validate_File_In_Stage"
   },
   "outputs": [],
   "source": [
    "-- Verify PDF was properly uploaded\n",
    "LIST @docs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ac735-69a5-4b52-980b-d16b6369a833",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Create_Parsed_PDF_Table"
   },
   "outputs": [],
   "source": [
    "-- This table will store the text from the parsed PDF\n",
    "CREATE OR REPLACE TABLE PARSED_PDFS ( \n",
    "    RELATIVE_PATH VARCHAR,\n",
    "    SIZE NUMBER(38,0),\n",
    "    FILE_URL VARCHAR,\n",
    "    PARSED_DATA VARCHAR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4433273-b96e-460b-bf04-706a4d5e0661",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Parse_PDFs"
   },
   "outputs": [],
   "source": [
    "-- We use Snowflake Cortex's PARSE_DOCUMENT function to extract the text from the pdf and save it to a column\n",
    "INSERT INTO PARSED_PDFS (relative_path, size, file_url, parsed_data)\n",
    "SELECT \n",
    "        relative_path,\n",
    "        size,\n",
    "        file_url,\n",
    "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@docs', relative_path, { 'mode': 'OCR' }):content AS parsed_data\n",
    "    FROM directory(@docs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee574655-7368-41c8-8fcb-eaaeae3ceff0",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Verify_PDF_Parsing"
   },
   "outputs": [],
   "source": "-- Verify the data was successfully parsed\nselect * from PARSED_PDFS;"
  },
  {
   "cell_type": "markdown",
   "id": "4be61e87-16c1-4406-9209-321a1d2363c9",
   "metadata": {
    "collapsed": false,
    "name": "Part2"
   },
   "source": [
    "## Part 2 - Generate Embeddings\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Explore various strategies for chunking the text data\n",
    "2. Generate embeddings for our text chunks\n",
    "3. Load the results into a Snowflake table using the `VECTOR` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36ab18-e357-41e5-995e-52b2a79f4d30",
   "metadata": {
    "collapsed": false,
    "name": "Chunking_Strategies"
   },
   "source": [
    "### Chunking Strategies\n",
    "\n",
    "In this section, we'll explore various chunking strategies. The right strategy will ultimately depend on the data and use case at hand. In our example, the PDF is cleanly delineated into paragraphs, so a simple regex based chunker is ideal.\n",
    "\n",
    "1. Snowflake Recursive Text Splitter\n",
    "2. Semantic Chunking\n",
    "3. Simple Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d7283-ccd9-4c5d-a2d0-4326aef5708c",
   "metadata": {
    "language": "sql",
    "name": "Snowflake_Recursive_Chunker"
   },
   "outputs": [],
   "source": [
    "-- Test out Snowflake's built in recursive text chunker\n",
    "SELECT\n",
    "  f.value::string AS chunk\n",
    "FROM\n",
    "  PARSED_PDFS,\n",
    "  LATERAL FLATTEN(\n",
    "    INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
    "      PARSED_DATA, -- column to split\n",
    "      'none', -- format ('none' or 'markdown')\n",
    "      1000, -- chunk size\n",
    "      100 -- overlap\n",
    "    )\n",
    "  ) f;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e3bf7-b2dc-4798-970f-cf5e9b067b9b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Simple_Chunking"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "parsed_data_df = session.table('parsed_pdfs')\n",
    "parsed_text = parsed_data_df.collect()[0]\n",
    "\n",
    "# Create chunks for each park and its project plan\n",
    "def is_title(line):\n",
    "    stripped = line.strip()\n",
    "    return (\n",
    "        bool(stripped) and \n",
    "        stripped == stripped.upper() and \n",
    "        any(c.isalpha() for c in stripped)\n",
    "    )\n",
    "\n",
    "def chunk_by_project(parsed_text):\n",
    "    lines = parsed_text[\"PARSED_DATA\"].splitlines()\n",
    "    chunks = []\n",
    "    current_title = None\n",
    "    current_desc_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        if is_title(line):\n",
    "            # Check if the next line is also a title (part of same heading)\n",
    "            title_lines = [line]\n",
    "            while i + 1 < len(lines) and is_title(lines[i + 1].strip()):\n",
    "                i += 1\n",
    "                title_lines.append(lines[i].strip())\n",
    "            # If we already have a title and description, save that chunk\n",
    "            if current_title:\n",
    "                chunk = f\"{current_title}\\n{' '.join(current_desc_lines).strip()}\"\n",
    "                chunks.append({\n",
    "                    \"relative_path\": parsed_text[\"RELATIVE_PATH\"],\n",
    "                    \"size\": parsed_text[\"SIZE\"],\n",
    "                    \"file_url\": parsed_text[\"FILE_URL\"],\n",
    "                    \"chunk\": chunk\n",
    "                })\n",
    "            # Start a new chunk\n",
    "            current_title = ' '.join(title_lines)\n",
    "            current_desc_lines = []\n",
    "        else:\n",
    "            current_desc_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_title and current_desc_lines:\n",
    "        chunk = f\"{current_title}\\n{' '.join(current_desc_lines).strip()}\"\n",
    "        chunks.append({\n",
    "            \"relative_path\": parsed_text[\"RELATIVE_PATH\"],\n",
    "            \"size\": parsed_text[\"SIZE\"],\n",
    "            \"file_url\": parsed_text[\"FILE_URL\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_by_project(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80620465-147e-42a5-9a46-5be0db654b5f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "View_Chunk_Results"
   },
   "outputs": [],
   "source": [
    "# View chunks\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f'chunk {idx}:', chunk['chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6172931-178e-4170-878d-53eb03e43b2a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Generate_Embeddings"
   },
   "outputs": [],
   "source": [
    "from snowflake.cortex import embed_text_768\n",
    "\n",
    "# Create embeddings for each chunk\n",
    "model = 'snowflake-arctic-embed-m-v1.5'\n",
    "for chunk in chunks:\n",
    "    chunk['embedding'] = embed_text_768(model, chunk['chunk'], session)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209153f-9804-4dc1-9d79-4b37ff093a8d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Load_Embeddings"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import VectorType, DoubleType\n",
    "\n",
    "# Save off chunks and embeddings into new table\n",
    "df = session.create_dataframe(chunks)\n",
    "df = df.with_column('embedding', df.col('embedding').cast(VectorType(float, 768)))\n",
    "df.write.save_as_table(\"DOCS_CHUNKS_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e4a58-2caa-48a3-8968-dd8981b244c4",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Verify_Embeddings"
   },
   "outputs": [],
   "source": [
    "-- Validate table\n",
    "select chunk, embedding from DOCS_CHUNKS_TABLE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b337bb-6f95-43db-b26f-733c98a3f6a3",
   "metadata": {
    "collapsed": false,
    "name": "Part_3"
   },
   "source": [
    "## Part 3: Test out different search methods\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Perform a standard cosine similarity search\n",
    "2. Create a Cortex Search Service\n",
    "3. Perform a search against the Cortex Search Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e1a83-ddd1-47b9-aae2-08d6648c8c3a",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Standard_Vector_Search"
   },
   "outputs": [],
   "source": [
    "-- Ault Park Trail\n",
    "SELECT VECTOR_COSINE_SIMILARITY(\n",
    "            docs_chunks_table.embedding,\n",
    "            SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m-v1.5', 'When will the Ault Park trail plan complete?')\n",
    "       ) as similarity,\n",
    "       chunk\n",
    "FROM docs_chunks_table\n",
    "ORDER BY similarity desc\n",
    "LIMIT 10\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a54785-f0e9-48cc-8b87-bde2476a0960",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Create_Cortex_Search_Service"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE parks_search_service\n",
    "  ON CHUNK\n",
    "  WAREHOUSE = compute_wh\n",
    "  TARGET_LAG = '1 day'\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-m-v1.5'\n",
    "  AS (\n",
    "    SELECT\n",
    "        CHUNK,\n",
    "    FROM docs_chunks_table\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9276996-bca5-4498-9229-cf6739802f21",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Cortex_Search_Results"
   },
   "outputs": [],
   "source": "# Quick test of the search service\nimport json\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.core import Root\n\nsession = get_active_session()\n\nroot = Root(session)\nparks_search_service = (root\n  .databases[\"GAIG_LAB\"]\n  .schemas[\"PUBLIC\"]\n  .cortex_search_services[\"parks_search_service\"]\n)\n\nresp = parks_search_service.search(\n  query=\"When will the Ault Park trail plan complete?\",\n  columns=[\"chunk\"],\n  limit=3\n)\n\nresults = json.loads(resp.to_json())['results']\n\nfor idx, chunk in enumerate(results):\n    print(f'Result: {idx+1}')\n    print(chunk['chunk'])"
  },
  {
   "cell_type": "markdown",
   "id": "82843258-5703-4c77-a76e-2be53470c09b",
   "metadata": {
    "collapsed": false,
    "name": "Part_4"
   },
   "source": [
    "## Part 4: Evals\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Build out a sample of questions and ground truth results\n",
    "2. Create a framework for running our samples through both standard search and Cortex search\n",
    "3. Perform the evaluation and display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d196d1-b06d-4031-a876-3fd0cb692882",
   "metadata": {
    "language": "python",
    "name": "Create_Eval_Set"
   },
   "outputs": [],
   "source": [
    "eval_set = [\n",
    "    {\n",
    "        \"query\": \"When will the Ault Park trail plan complete?\",\n",
    "        \"expected_chunk\": \"AULT PARK VALLEY TRAIL One of the busiest trails in Cincinnati Parks is experiencing serious erosion issues along the creek also housing important sewer infrastructure. This project shores up the trail to keep hikers safe in advance of a larger MSD sewer project in the coming years to protect the trail in the long term. The project is underway and will close out in early 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"When was the Sawyer Point Park playground burnt down?\",\n",
    "        \"expected_chunk\": \"SAWYER POINT PLAYGROUND AND PARK PLANNING Work is underway to restore the Sawyer Point Park playground, which was suddenly destroyed by a massive fire in November 2024. This project represents a chance to create an amazing new, uniquely Cincinnati, amenity for the next generation of park users of a wide range of ages and abilities to enjoy. The new playground will be built in the vicinity of the former playground though not in the same location. The goal is to engage with the community to develop something truly fantastic in this iconic regional park serving as a distinctive source of lasting pride for our city. The project also creates an opportunity to comprehensively review the layout of the park to guide a longer-term plan for improvements in the coming years.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How many miles is the CROWN network?\",\n",
    "        \"expected_chunk\": \"BRAMBLE PARK TRAIL This project is a partnership with the community and will utilize a State of Ohio Department of Natural Resources grant. This will be the first segment of the Little Duck Creek Trail and run 0.35 miles in length through Bramble Park in Madisonville. The trail will connect to the Murray Trail, part of the CROWN network connecting more than 104 miles of trails in Cincinnati. Planning will take place during 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"When did Smale park open?\",\n",
    "        \"expected_chunk\": \"SMALE CONCRETE & GRANITE UPGRADES This award-winning, heavily used signature Cincinnati Park opened in 2012. Sections of concrete and specialized granite need repair in order to maintain this regional asset.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which park was added to the National Register of Historic Places?\",\n",
    "        \"expected_chunk\": \"GIBSON HOUSE ROOF & FAÇADE This architecturally important structure was built in the middle 19th century and added to the National Register of Historic Places in 1976. Critical repairs are needed to preserve this treasure, which is now used for offices and a rental venue. Construction is planned to begin early 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who is the Park Board partnering with for the Smale River's Edge project?\",\n",
    "        \"expected_chunk\": \"SMALE RIVER’S EDGE The U.S. Army Corps of Engineers and the Cincinnati Park Board are partnering on a study to improve and revitalize the Cincinnati Ohio River’s edge along the western edge of Smale Riverfront Park. The overall vision is to make the Cincinnati Riverfront a welcoming, safe, sustainable park, serving as a gateway to connect people to their heritage, community, and the natural environment for generations to come. The project will provide opportunities for ecosystem restoration and recreation, while protecting Cincinnati’s Riverfront from erosion. Initial design selection of this multi-million project will be complete in mid-2025 with construction planned to start in 2027.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How long will the California Woods Hydrological plan take?\",\n",
    "        \"expected_chunk\": \"CALIFORNIA WOODS HYDROLOGICAL PLAN DESIGN This amazing preserve is experiencing significant erosion issues that threaten long-term public access to the park. A specialized firm has been selected to develop a plan for sustainable interventions to the stream flow to mitigate the on-going erosion issues in the most environmentally sustainable manner. Investigation and design work begins in the second quarter of 2025 and will take about a year to finalize.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which park plan is partering with the Cincinnati Off Road Alliance?\",\n",
    "        \"expected_chunk\": \"MT. AIRY BIKE SKILLS COURSE This partnership with the Cincinnati Parks Foundation and the Cincinnati Off Road Alliance (CORA) will nearly double the existing mileage of mountain biking trails within Mt. Airy Forest. It will be the first beginner natural surface trail experience within the city. With input from the community, the project has been funded and a contractor selected. The project is anticipated to be complete in early 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How many acres is the Cincinnati Park system comprised of?\",\n",
    "        \"expected_chunk\": \"CINCINNAT PARKS PARK IMPROVEMENT PROJECTS 3-YEAR PLAN Cincinnati Parks' 5,000 acres consist of 8 regional parks, 70 neighborhood parks, 34 preserves and natural areas, 5 parkways, 65 miles of hiking trails, 80,000 street trees on 1,000 miles of City streets, 6 nature centers, 18 scenic overlooks, 52 playgrounds, 500 landscaped gardens, and over 100 picnic areas. With all of this to care for, there are constant needs of all shapes and sizes. Whether it be a bad sidewalk, an aging playground, a leaking roof, or a park that could use a complete facelift, there’s plenty to do to keep our parks looking great and best serving our residents and users. This is why the Board of Park Commissioners approved a work plan, generated by Parks staff, outlining projects underway and planned over the next 3 years. This plan represents a roadmap of what Cincinnati Parks will be prioritizing in the coming years and creates transparency into improvement projects. This was developed after a careful evaluation based on a number of factors including safety, equity, efficiencies, long-term maintenance, available funding, and more. This plan represents current priorities, capacity, and needs, and is a living document that will be updated as circumstances evolve and schedules are adjusted.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which communities does the Burnet Woods dog park serve?\",\n",
    "        \"expected_chunk\": \"BURNET WOODS DOG PARK This new community dog park will serve Clifton, Corryville, CUF, and the surrounding areas, further contributing to the attractiveness and quality of life. The project represents a partnership with a number of community supporters, partners, and donors, including the Cincinnati Parks Foundation and Clifton Pop-up-Pup-Party (PUPP). The new amenity is expected to be under construction in May 2025 and take about 3 months to complete.\"\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4707b-c54b-41f1-9541-4176eaa6e4b2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Eval_Setup"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\nimport json\nimport re\n\nsession = Session.builder.getOrCreate()\nmodel = 'snowflake-arctic-embed-m-v1.5'\n\ndef escape_sql_string(s):\n    return s.replace(\"'\", \"''\")\n\ndef vector_search(query, k=3):\n    safe_query = escape_sql_string(query)\n    return session.sql(f\"\"\"\n        SELECT chunk, VECTOR_COSINE_SIMILARITY(\n            docs_chunks_table.embedding,\n            SNOWFLAKE.CORTEX.EMBED_TEXT_768('{model}', '{safe_query}')\n        ) AS similarity\n        FROM docs_chunks_table\n        ORDER BY similarity DESC\n        LIMIT {k}\n    \"\"\").collect()\n\ndef cortex_search(query, limit=3):\n    parks_search_service = (root.databases[\"GAIG_LAB\"]\n                                   .schemas[\"PUBLIC\"]\n                                   .cortex_search_services[\"parks_search_service\"])\n    resp = parks_search_service.search(\n        query=query,\n        columns=[\"chunk\"],\n        limit=limit\n    )\n    results = json.loads(resp.to_json())['results']\n    return results\n\n\ndef normalize(text):\n    # Lowercase, remove extra whitespace, and normalize newlines\n    return re.sub(r'\\s+', ' ', text.strip().lower())\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6ead9-7089-4d4c-b0cd-7be833fe549e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Run_Evaluation"
   },
   "outputs": [],
   "source": [
    "def run_eval():\n",
    "    results = []\n",
    "    \n",
    "    for item in eval_set:\n",
    "        query = item['query']\n",
    "        expected = item['expected_chunk']\n",
    "    \n",
    "        # Vector search\n",
    "        vector_results = vector_search(query, k=3)\n",
    "        vector_chunks = [r['CHUNK'] for r in vector_results]\n",
    "        vector_match_rank = next(\n",
    "            (i + 1 for i, chunk in enumerate(vector_chunks)\n",
    "             if normalize(chunk) == normalize(expected)),\n",
    "            None\n",
    "        )\n",
    "    \n",
    "        # Cortex search\n",
    "        cortex_results = cortex_search(query, limit=3)\n",
    "        cortex_chunks = [r['chunk'] for r in cortex_results]\n",
    "        cortex_match_rank = next(\n",
    "            (i + 1 for i, chunk in enumerate(cortex_chunks)\n",
    "             if normalize(chunk) == normalize(expected)),\n",
    "            None\n",
    "        )\n",
    "    \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"expected_chunk\": expected,\n",
    "            \"vector_hit_in_top3\": vector_match_rank is not None,\n",
    "            \"vector_hit_rank\": vector_match_rank or \"Miss\",\n",
    "            \"vector_chunks\": vector_chunks,\n",
    "            \"cortex_hit_in_top3\": cortex_match_rank is not None,\n",
    "            \"cortex_hit_rank\": cortex_match_rank or \"Miss\",\n",
    "            \"cortex_chunks\": cortex_chunks\n",
    "        })\n",
    "    return results\n",
    "\n",
    "results = run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddf8ec-7169-45bd-8a4a-5f1d98b150a1",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "View_Eval_Results"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df[['query', 'vector_hit_in_top3', 'cortex_hit_in_top3']]"
   ]
  },
  {
   "cell_type": "code",
   "id": "a494e011-568c-4f94-8d1e-8cd7300e9162",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}