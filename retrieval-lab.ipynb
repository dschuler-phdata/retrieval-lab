{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f02c0edb-d8af-448e-94e9-17367d84e55d",
   "metadata": {
    "collapsed": false,
    "name": "Overview"
   },
   "source": [
    "# Retrieval Hands On Lab\n",
    "\n",
    "## Objectives\n",
    "By the end of this lab, participants will:\n",
    "\n",
    "1. Understand how to parse PDFs inside Snowflake\n",
    "2. Understand how to create vector representations of text data and load it into Snowflake tables\n",
    "3. Perform similarity search against embeddings in Snowflake\n",
    "4. Use Snowflake Cortex Search for retrieval and understand the benefits compared to simple similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065f51a-a7a7-4a0d-a686-208956c9639e",
   "metadata": {
    "collapsed": false,
    "name": "Part_1"
   },
   "source": [
    "# Part 1: Setup\n",
    "In this section, we will:\n",
    "\n",
    "1. Create some snowflake objects to store our data in\n",
    "2. Upload a PDF of Cincinnati Parks' 3 year development plan into a stage\n",
    "3. Parse the PDF into usable text and load the results into a Snowflake table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397eb24-3ccf-4c42-ae1b-a9f797a98758",
   "metadata": {
    "collapsed": false,
    "name": "Inspect_PDF"
   },
   "source": [
    "## Inspect the PDF\n",
    "\n",
    "Download the Cincinnati Parks 3 year plan by clicking on the data directory, then clicking the '...' next to the PDF and choosing 'Download'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd5177-ff2e-4e69-95b3-3f28e9010938",
   "metadata": {
    "collapsed": false,
    "name": "Add_Python_Packages"
   },
   "source": [
    "## Add python packages\n",
    "\n",
    "Click on 'Packages' at the top of the notebook and add the following packages:\n",
    "\n",
    "- snowflake.core\n",
    "- snowflake-snowpark-python\n",
    "- snowflake-ml-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "sql",
    "name": "Create_Snowflake_Objects"
   },
   "outputs": [],
   "source": [
    "USE RETRIEVAL_LAB.PUBLIC;\n",
    "\n",
    "-- Create a stage to store our PDF\n",
    "CREATE OR REPLACE STAGE docs ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = ( ENABLE = true );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d9541-9e4a-4446-a604-3dd5ef5d3969",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Put_PDF_In_Stage"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "session = get_active_session()\n",
    "pdf_path = \"./data/cincinnati-parks-3-year-plan.pdf\"\n",
    "\n",
    "session.file.put(\n",
    "    pdf_path,\n",
    "    \"@docs\",\n",
    "    auto_compress=False,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Force a refresh on the stage so the doc is accessible\n",
    "session.sql(\"ALTER STAGE docs REFRESH\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500453d9-a3f9-4e93-93dc-88f6b16df299",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Validate_File_In_Stage"
   },
   "outputs": [],
   "source": [
    "-- Verify PDF was properly uploaded\n",
    "LIST @docs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ac735-69a5-4b52-980b-d16b6369a833",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Create_Parsed_PDF_Table"
   },
   "outputs": [],
   "source": [
    "-- This table will store the text from the parsed PDF\n",
    "CREATE OR REPLACE TABLE PARSED_PDFS ( \n",
    "    RELATIVE_PATH VARCHAR,\n",
    "    SIZE NUMBER(38,0),\n",
    "    FILE_URL VARCHAR,\n",
    "    PARSED_DATA VARCHAR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4433273-b96e-460b-bf04-706a4d5e0661",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Parse_PDFs"
   },
   "outputs": [],
   "source": [
    "-- We use Snowflake Cortex's PARSE_DOCUMENT function to extract the text from the pdf and save it to a column\n",
    "INSERT INTO PARSED_PDFS (relative_path, size, file_url, parsed_data)\n",
    "SELECT \n",
    "        relative_path,\n",
    "        size,\n",
    "        file_url,\n",
    "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT('@docs', relative_path, { 'mode': 'OCR' }):content AS parsed_data\n",
    "    FROM directory(@docs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee574655-7368-41c8-8fcb-eaaeae3ceff0",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Verify_PDF_Parsing"
   },
   "outputs": [],
   "source": [
    "-- Verify the data was successfully parsed\n",
    "select * from PARSED_PDFS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be61e87-16c1-4406-9209-321a1d2363c9",
   "metadata": {
    "collapsed": false,
    "name": "Part2"
   },
   "source": [
    "## Part 2 - Generate Embeddings\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Explore various strategies for chunking the text data\n",
    "2. Generate embeddings for our text chunks\n",
    "3. Load the results into a Snowflake table using the `VECTOR` datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36ab18-e357-41e5-995e-52b2a79f4d30",
   "metadata": {
    "collapsed": false,
    "name": "Chunking_Strategies"
   },
   "source": [
    "### Chunking Strategies\n",
    "\n",
    "In this section, we'll explore various chunking strategies. The right strategy will ultimately depend on the data and use case at hand. The three strategies explored in this section are:\n",
    "\n",
    "1. **Snowflake's Recursive Text Splitter** - this method uses a set of separators to iteratively split the text. If the result of the split for the primary separator doesn't result in the desired chunk size, another pass will be taken through the text with the next separator. Snowflake's default separator list is `[”\\n\\n”, “\\n”, “ “, “”]`, meaning it will first use a paragraph break, then a line break, then a space, and then an empty string. This can be altered as needed.\n",
    "\n",
    "2. **Semantic Chunking** - this strategy attempts to generate chunks by accounting for the semantic meaning of text within the document. The goal is to create chunks comprised of sentences that cover the same theme or topic. This will result in more varied chunk sizes as opposed to always creating a chunk size of 500 tokens, as an example.\n",
    "\n",
    "3. Paragraph Chunking - this is a simpler strategy that can be used on documents that are well structured into paragraphs. In our case, this strategy makes a lot of sense as it will group each park plan into its own chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd35ad8-6208-4cfd-ac5a-d11573d12626",
   "metadata": {
    "language": "python",
    "name": "Snowflake_Recursive_Chunker"
   },
   "outputs": [],
   "source": [
    "chunking_sql = \"\"\"\n",
    "    SELECT f.value::string AS chunk\n",
    "    FROM PARSED_PDFS,\n",
    "    LATERAL FLATTEN(\n",
    "      INPUT => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(\n",
    "      PARSED_DATA, -- column to split\n",
    "      'none', -- format ('none' or 'markdown')\n",
    "      1000, -- chunk size\n",
    "      100 -- overlap\n",
    "      )\n",
    "    ) f\n",
    "\"\"\"\n",
    "\n",
    "recursive_chunks = [row[\"CHUNK\"] for row in session.sql(chunking_sql).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905dc98-c538-47a8-97da-3eb4f5ab8d32",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Semantic_Chunking"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from snowflake.cortex import embed_text_768\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "\n",
    "class SnowflakeCortexEmbeddings(Embeddings):\n",
    "    def __init__(self):\n",
    "        self.session = get_active_session()\n",
    "        self.model = 'snowflake-arctic-embed-m-v1.5'\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            embedding = embed_text_768(self.model, text, self.session)\n",
    "            results.append(embedding)\n",
    "        return results\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "\n",
    "snowflake_embeddings = SnowflakeCortexEmbeddings()\n",
    "chunker = SemanticChunker(embeddings=snowflake_embeddings)\n",
    "\n",
    "parsed_data_df = session.table('parsed_pdfs')\n",
    "parsed_text = parsed_data_df.collect()[0]\n",
    "\n",
    "# Removing line breaks to pass in a continuous string to the chunker\n",
    "cleaned_text = re.sub(r'\\n(?=\\S)', ' ', parsed_text[\"PARSED_DATA\"])\n",
    "\n",
    "chunks = chunker.create_documents([cleaned_text])\n",
    "semantic_chunks = [doc.page_content for doc in chunks]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e3bf7-b2dc-4798-970f-cf5e9b067b9b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Paragraph_Chunking"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "parsed_data_df = session.table('parsed_pdfs')\n",
    "parsed_text = parsed_data_df.collect()[0]\n",
    "\n",
    "# Create chunks for each park and its project plan\n",
    "def is_title(line):\n",
    "    stripped = line.strip()\n",
    "    return (\n",
    "        bool(stripped) and \n",
    "        stripped == stripped.upper() and \n",
    "        any(c.isalpha() for c in stripped)\n",
    "    )\n",
    "\n",
    "def chunk_by_project(parsed_text):\n",
    "    lines = parsed_text[\"PARSED_DATA\"].splitlines()\n",
    "    chunks = []\n",
    "    current_title = None\n",
    "    current_desc_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        if is_title(line):\n",
    "            # Check if the next line is also a title (part of same heading)\n",
    "            title_lines = [line]\n",
    "            while i + 1 < len(lines) and is_title(lines[i + 1].strip()):\n",
    "                i += 1\n",
    "                title_lines.append(lines[i].strip())\n",
    "            # If we already have a title and description, save that chunk\n",
    "            if current_title:\n",
    "                chunk = f\"{current_title}\\n{' '.join(current_desc_lines).strip()}\"\n",
    "                chunks.append({\n",
    "                    \"relative_path\": parsed_text[\"RELATIVE_PATH\"],\n",
    "                    \"size\": parsed_text[\"SIZE\"],\n",
    "                    \"file_url\": parsed_text[\"FILE_URL\"],\n",
    "                    \"chunk\": chunk\n",
    "                })\n",
    "            # Start a new chunk\n",
    "            current_title = ' '.join(title_lines)\n",
    "            current_desc_lines = []\n",
    "        else:\n",
    "            current_desc_lines.append(line)\n",
    "        i += 1\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_title and current_desc_lines:\n",
    "        chunk = f\"{current_title}\\n{' '.join(current_desc_lines).strip()}\"\n",
    "        chunks.append({\n",
    "            \"relative_path\": parsed_text[\"RELATIVE_PATH\"],\n",
    "            \"size\": parsed_text[\"SIZE\"],\n",
    "            \"file_url\": parsed_text[\"FILE_URL\"],\n",
    "            \"chunk\": chunk\n",
    "        })\n",
    "\n",
    "    return chunks\n",
    "\n",
    "paragraph_chunks = chunk_by_project(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e07375-caee-4dbe-9255-6daa5f43921d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Compare_Chunking_Results"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # This runs a SQL query to tokenize the text and count tokens\n",
    "    result = session.sql(f\"\"\"\n",
    "        SELECT SNOWFLAKE.CORTEX.COUNT_TOKENS('snowflake-arctic-embed-m', $${text}$$) AS token_count\n",
    "    \"\"\").collect()\n",
    "    return result[0][\"TOKEN_COUNT\"]\n",
    "\n",
    "def get_summary_statistics(chunks, label):\n",
    "    token_counts = [count_tokens(chunk if isinstance(chunk, str) else chunk[\"chunk\"]) for chunk in chunks]\n",
    "    return {\n",
    "        \"strategy\": label,\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"min_tokens\": min(token_counts),\n",
    "        \"max_tokens\": max(token_counts),\n",
    "        \"avg_tokens\": sum(token_counts) / len(token_counts) if chunks else 0,\n",
    "    }\n",
    "\n",
    "all_summaries = [\n",
    "    get_summary_statistics(recursive_chunks, \"recursive\"),\n",
    "    get_summary_statistics(semantic_chunks, \"semantic\"),\n",
    "    get_summary_statistics(paragraph_chunks, \"paragraph\")\n",
    "]\n",
    "\n",
    "# Output as DataFrame for readability\n",
    "summary_df = pd.DataFrame(all_summaries)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80620465-147e-42a5-9a46-5be0db654b5f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "View_Chunk_Results"
   },
   "outputs": [],
   "source": "# View paragraph chunks\nfor idx, chunk in enumerate(paragraph_chunks):\n    if idx < 10:\n        print(f'chunk {idx}:', chunk['chunk'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6172931-178e-4170-878d-53eb03e43b2a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Generate_Embeddings"
   },
   "outputs": [],
   "source": [
    "from snowflake.cortex import embed_text_768\n",
    "\n",
    "# Create embeddings for each chunk\n",
    "model = 'snowflake-arctic-embed-m-v1.5'\n",
    "for chunk in paragraph_chunks:\n",
    "    chunk['embedding'] = embed_text_768(model, chunk['chunk'], session)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209153f-9804-4dc1-9d79-4b37ff093a8d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Load_Embeddings"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import VectorType, DoubleType\n",
    "\n",
    "# Save off chunks and embeddings into new table\n",
    "df = session.create_dataframe(paragraph_chunks)\n",
    "df = df.with_column('embedding', df.col('embedding').cast(VectorType(float, 768)))\n",
    "df.write.save_as_table(\"DOCS_CHUNKS_TABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e4a58-2caa-48a3-8968-dd8981b244c4",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Verify_Embeddings"
   },
   "outputs": [],
   "source": [
    "-- Validate table\n",
    "select chunk, embedding from DOCS_CHUNKS_TABLE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b337bb-6f95-43db-b26f-733c98a3f6a3",
   "metadata": {
    "collapsed": false,
    "name": "Part_3"
   },
   "source": [
    "## Part 3: Test out different search methods\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "1. Perform a standard cosine similarity search\n",
    "2. Create a Cortex Search Service\n",
    "3. Perform a search against the Cortex Search Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2451524-1339-46d0-bc9b-e2bfb37bb5b8",
   "metadata": {
    "name": "Standard_Search_Desc",
    "collapsed": false
   },
   "source": "### Standard Similarity Search\nSnowflake offers a built in function to perform semantic similarity search. You provide the column of type `VECTOR` that you will be searching and the embedding of your search query. It will return the similarity score (between -1 and 1) for each embedding in your vector column."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e1a83-ddd1-47b9-aae2-08d6648c8c3a",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Standard_Vector_Search"
   },
   "outputs": [],
   "source": "-- Ault Park Trail\nSELECT VECTOR_COSINE_SIMILARITY(\n            docs_chunks_table.embedding,\n            SNOWFLAKE.CORTEX.EMBED_TEXT_768('snowflake-arctic-embed-m-v1.5', 'When will the Ault Park trail plan complete?')\n       ) as similarity,\n       chunk\nFROM docs_chunks_table\nORDER BY similarity desc\nLIMIT 10\n;"
  },
  {
   "cell_type": "markdown",
   "id": "bc84a433-8a02-40c8-9ea2-42430f043474",
   "metadata": {
    "name": "Cortex_Search_Desc",
    "collapsed": false
   },
   "source": "### Snowflake Cortex Search Service\n\nThe Cortex Search Service provides a simple way to perform search against your data. It handles embedding data, loading it into a table, and provides a low-latency interface for searching. The search method is a hybrid of keyword and vector search and also has a built-in semantic reranker to provide the most relevant chunks of data.\n\nAll of this together provides a simple mechanism for high quality, performant search inside Snowflake."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a54785-f0e9-48cc-8b87-bde2476a0960",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "Create_Cortex_Search_Service"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE CORTEX SEARCH SERVICE parks_search_service\n",
    "  ON CHUNK\n",
    "  WAREHOUSE = compute_wh\n",
    "  TARGET_LAG = '1 day'\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-m-v1.5'\n",
    "  AS (\n",
    "    SELECT\n",
    "        CHUNK,\n",
    "    FROM docs_chunks_table\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9276996-bca5-4498-9229-cf6739802f21",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Cortex_Search_Results"
   },
   "outputs": [],
   "source": [
    "# Quick test of the search service\n",
    "import json\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.core import Root\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "root = Root(session)\n",
    "parks_search_service = (root\n",
    "  .databases[\"RETRIEVAL_LAB\"]\n",
    "  .schemas[\"PUBLIC\"]\n",
    "  .cortex_search_services[\"parks_search_service\"]\n",
    ")\n",
    "\n",
    "resp = parks_search_service.search(\n",
    "  query=\"When will the Ault Park trail plan complete?\",\n",
    "  columns=[\"chunk\"],\n",
    "  limit=3\n",
    ")\n",
    "\n",
    "results = json.loads(resp.to_json())['results']\n",
    "\n",
    "for idx, chunk in enumerate(results):\n",
    "    print(f'Result: {idx+1}')\n",
    "    print(chunk['chunk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82843258-5703-4c77-a76e-2be53470c09b",
   "metadata": {
    "collapsed": false,
    "name": "Part_4"
   },
   "source": "## Part 4: Evals\n\nIn this section, we'll:\n\n1. Build out a sample of questions and ground truth results\n2. Create a framework for running our samples through both standard search and Cortex search\n3. Perform the evaluation and display the results"
  },
  {
   "cell_type": "markdown",
   "id": "1c03f03d-1263-47bd-9208-096641c884df",
   "metadata": {
    "name": "Eval_Overview",
    "collapsed": false
   },
   "source": "Typically, a framework such as [ragas](https://docs.ragas.io/en/stable/) would be used to perform the evaluation, but in order to keep the lab setup minimal, we are going to do the eval without the help of an external library.\n\n## Eval Metrics\nFor retrieval, there are a number of metrics to consider:\n\n- Precision@k - the ratio of relevant chunks retrieved to the total number of chunks retrieved\n- Recall@k - the ratio of relevant chunks retrieved to the total number of relevant chunks\n\nGiven we know exactly what chunk should be expected for a given search query, we can keep it simple and measure:\n\n- "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d196d1-b06d-4031-a876-3fd0cb692882",
   "metadata": {
    "language": "python",
    "name": "Create_Eval_Set"
   },
   "outputs": [],
   "source": [
    "eval_set = [\n",
    "    {\n",
    "        \"query\": \"When will the Ault Park trail plan complete?\",\n",
    "        \"expected_chunk\": \"AULT PARK VALLEY TRAIL One of the busiest trails in Cincinnati Parks is experiencing serious erosion issues along the creek also housing important sewer infrastructure. This project shores up the trail to keep hikers safe in advance of a larger MSD sewer project in the coming years to protect the trail in the long term. The project is underway and will close out in early 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"When was the Sawyer Point Park playground burnt down?\",\n",
    "        \"expected_chunk\": \"SAWYER POINT PLAYGROUND AND PARK PLANNING Work is underway to restore the Sawyer Point Park playground, which was suddenly destroyed by a massive fire in November 2024. This project represents a chance to create an amazing new, uniquely Cincinnati, amenity for the next generation of park users of a wide range of ages and abilities to enjoy. The new playground will be built in the vicinity of the former playground though not in the same location. The goal is to engage with the community to develop something truly fantastic in this iconic regional park serving as a distinctive source of lasting pride for our city. The project also creates an opportunity to comprehensively review the layout of the park to guide a longer-term plan for improvements in the coming years.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How many miles is the CROWN network?\",\n",
    "        \"expected_chunk\": \"BRAMBLE PARK TRAIL This project is a partnership with the community and will utilize a State of Ohio Department of Natural Resources grant. This will be the first segment of the Little Duck Creek Trail and run 0.35 miles in length through Bramble Park in Madisonville. The trail will connect to the Murray Trail, part of the CROWN network connecting more than 104 miles of trails in Cincinnati. Planning will take place during 2025.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"When did Smale park open?\",\n",
    "        \"expected_chunk\": \"SMALE CONCRETE & GRANITE UPGRADES This award-winning, heavily used signature Cincinnati Park opened in 2012. Sections of concrete and specialized granite need repair in order to maintain this regional asset.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which park was added to the National Register of Historic Places?\",\n",
    "        \"expected_chunk\": \"GIBSON HOUSE ROOF & FAÇADE This architecturally important structure was built in the middle 19th century and added to the National Register of Historic Places in 1976. Critical repairs are needed to preserve this treasure, which is now used for offices and a rental venue. Construction is planned to begin early 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who is the Park Board partnering with for the Smale River's Edge project?\",\n",
    "        \"expected_chunk\": \"SMALE RIVER’S EDGE The U.S. Army Corps of Engineers and the Cincinnati Park Board are partnering on a study to improve and revitalize the Cincinnati Ohio River’s edge along the western edge of Smale Riverfront Park. The overall vision is to make the Cincinnati Riverfront a welcoming, safe, sustainable park, serving as a gateway to connect people to their heritage, community, and the natural environment for generations to come. The project will provide opportunities for ecosystem restoration and recreation, while protecting Cincinnati’s Riverfront from erosion. Initial design selection of this multi-million project will be complete in mid-2025 with construction planned to start in 2027.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How long will the California Woods Hydrological plan take?\",\n",
    "        \"expected_chunk\": \"CALIFORNIA WOODS HYDROLOGICAL PLAN DESIGN This amazing preserve is experiencing significant erosion issues that threaten long-term public access to the park. A specialized firm has been selected to develop a plan for sustainable interventions to the stream flow to mitigate the on-going erosion issues in the most environmentally sustainable manner. Investigation and design work begins in the second quarter of 2025 and will take about a year to finalize.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which park plan is partering with the Cincinnati Off Road Alliance?\",\n",
    "        \"expected_chunk\": \"MT. AIRY BIKE SKILLS COURSE This partnership with the Cincinnati Parks Foundation and the Cincinnati Off Road Alliance (CORA) will nearly double the existing mileage of mountain biking trails within Mt. Airy Forest. It will be the first beginner natural surface trail experience within the city. With input from the community, the project has been funded and a contractor selected. The project is anticipated to be complete in early 2026.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How many acres is the Cincinnati Park system comprised of?\",\n",
    "        \"expected_chunk\": \"CINCINNAT PARKS PARK IMPROVEMENT PROJECTS 3-YEAR PLAN Cincinnati Parks' 5,000 acres consist of 8 regional parks, 70 neighborhood parks, 34 preserves and natural areas, 5 parkways, 65 miles of hiking trails, 80,000 street trees on 1,000 miles of City streets, 6 nature centers, 18 scenic overlooks, 52 playgrounds, 500 landscaped gardens, and over 100 picnic areas. With all of this to care for, there are constant needs of all shapes and sizes. Whether it be a bad sidewalk, an aging playground, a leaking roof, or a park that could use a complete facelift, there’s plenty to do to keep our parks looking great and best serving our residents and users. This is why the Board of Park Commissioners approved a work plan, generated by Parks staff, outlining projects underway and planned over the next 3 years. This plan represents a roadmap of what Cincinnati Parks will be prioritizing in the coming years and creates transparency into improvement projects. This was developed after a careful evaluation based on a number of factors including safety, equity, efficiencies, long-term maintenance, available funding, and more. This plan represents current priorities, capacity, and needs, and is a living document that will be updated as circumstances evolve and schedules are adjusted.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Which communities does the Burnet Woods dog park serve?\",\n",
    "        \"expected_chunk\": \"BURNET WOODS DOG PARK This new community dog park will serve Clifton, Corryville, CUF, and the surrounding areas, further contributing to the attractiveness and quality of life. The project represents a partnership with a number of community supporters, partners, and donors, including the Cincinnati Parks Foundation and Clifton Pop-up-Pup-Party (PUPP). The new amenity is expected to be under construction in May 2025 and take about 3 months to complete.\"\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4707b-c54b-41f1-9541-4176eaa6e4b2",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "Eval_Setup"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.functions import col\n",
    "import json\n",
    "import re\n",
    "\n",
    "session = Session.builder.getOrCreate()\n",
    "model = 'snowflake-arctic-embed-m-v1.5'\n",
    "\n",
    "def escape_sql_string(s):\n",
    "    return s.replace(\"'\", \"''\")\n",
    "\n",
    "def vector_search(query, k=3):\n",
    "    safe_query = escape_sql_string(query)\n",
    "    return session.sql(f\"\"\"\n",
    "        SELECT chunk, VECTOR_COSINE_SIMILARITY(\n",
    "            docs_chunks_table.embedding,\n",
    "            SNOWFLAKE.CORTEX.EMBED_TEXT_768('{model}', '{safe_query}')\n",
    "        ) AS similarity\n",
    "        FROM docs_chunks_table\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT {k}\n",
    "    \"\"\").collect()\n",
    "\n",
    "def cortex_search(query, limit=3):\n",
    "    parks_search_service = (root.databases[\"RETRIEVAL_LAB\"]\n",
    "                                   .schemas[\"PUBLIC\"]\n",
    "                                   .cortex_search_services[\"parks_search_service\"])\n",
    "    resp = parks_search_service.search(\n",
    "        query=query,\n",
    "        columns=[\"chunk\"],\n",
    "        limit=limit\n",
    "    )\n",
    "    results = json.loads(resp.to_json())['results']\n",
    "    return results\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    # Lowercase, remove extra whitespace, and normalize newlines\n",
    "    return re.sub(r'\\s+', ' ', text.strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b34b9f8e-d836-487d-b204-75f62a69e8cc",
   "metadata": {
    "language": "python",
    "name": "Define_Eval_Functions"
   },
   "outputs": [],
   "source": "import time\n\ndef run_eval():\n    results = []\n\n    for item in eval_set:\n        query = item['query']\n        expected = item['expected_chunk']\n\n        # Vector search with timing\n        start_time = time.time()\n        vector_results = vector_search(query, k=3)\n        vector_time = time.time() - start_time\n        vector_chunks = [r['CHUNK'] for r in vector_results]\n        vector_match_rank = next(\n            (i + 1 for i, chunk in enumerate(vector_chunks)\n             if normalize(chunk) == normalize(expected)),\n            None\n        )\n\n        # Cortex search with timing\n        start_time = time.time()\n        cortex_results = cortex_search(query, limit=3)\n        cortex_time = time.time() - start_time\n        cortex_chunks = [r['chunk'] for r in cortex_results]\n        cortex_match_rank = next(\n            (i + 1 for i, chunk in enumerate(cortex_chunks)\n             if normalize(chunk) == normalize(expected)),\n            None\n        )\n\n        results.append({\n            \"query\": query,\n            \"expected_chunk\": expected,\n            \"vector_hit_rank\": vector_match_rank or \"Miss\",\n            \"vector_chunks\": vector_chunks,\n            \"vector_time\": vector_time,\n            \"cortex_hit_rank\": cortex_match_rank or \"Miss\",\n            \"cortex_chunks\": cortex_chunks,\n            \"cortex_time\": cortex_time\n        })\n\n    return results\n\n\nimport numpy as np\n\ndef compute_metrics(results, method):\n    total = len(results)\n    hit_at_1 = 0\n    hit_at_3 = 0\n    reciprocal_ranks = []\n    ranks = []\n    misses = 0\n    times = []\n\n    for r in results:\n        hit_rank = r[f\"{method}_hit_rank\"]\n        response_time = r[f\"{method}_time\"]\n        times.append(response_time)\n\n        if isinstance(hit_rank, int):\n            if hit_rank == 1:\n                hit_at_1 += 1\n            if hit_rank <= 3:\n                hit_at_3 += 1\n            reciprocal_ranks.append(1.0 / hit_rank)\n            ranks.append(hit_rank)\n        else:\n            misses += 1\n            reciprocal_ranks.append(0.0)\n\n    accuracy = hit_at_1 / total\n    hit3 = hit_at_3 / total\n    mrr = sum(reciprocal_ranks) / total\n    mean_rank = sum(ranks) / len(ranks) if ranks else None\n    miss_rate = misses / total\n    avg_time = np.mean(times)\n\n    p50 = np.percentile(times, 50)\n    p90 = np.percentile(times, 90)\n    p99 = np.percentile(times, 99)\n\n    return {\n        \"Accuracy (Hit@1)\": round(accuracy, 3),\n        \"Hit@3\": round(hit3, 3),\n        \"MRR\": round(mrr, 3),\n        \"Mean Rank\": round(mean_rank, 2) if mean_rank is not None else \"N/A\",\n        \"Miss Rate\": round(miss_rate, 3),\n        \"Avg Retrieval Time (ms)\": round(avg_time * 1000, 3),\n        \"P50 Latency (ms)\": round(p50 * 1000, 3),\n        \"P90 Latency (ms)\": round(p90 * 1000, 3),\n        \"P99 Latency (ms)\": round(p99 * 1000, 3)\n    }\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21487d7e-867c-48dd-8ab4-5109facbb828",
   "metadata": {
    "language": "python",
    "name": "Run_Eval",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "results = run_eval()\n\nvector_metrics = compute_metrics(results, method=\"vector\")\ncortex_metrics = compute_metrics(results, method=\"cortex\")\n\ndf = pd.DataFrame({\n    \"Metric\": list(vector_metrics.keys()),\n    \"Vector Search\": list(vector_metrics.values()),\n    \"Cortex Search\": list(cortex_metrics.values())\n})\n\ndf",
   "execution_count": null
  }
 ]
}
